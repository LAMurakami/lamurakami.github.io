<html><head>

<title>(AWS) Elastic File System (EFS)</title>

<link rel="Shortcut Icon" href="../LAM.ico" />
<link rel="stylesheet" type="text/css" href="../Style.css" />

</head><body>

<!-- START top_nav menuwrapper menuBar -->
<div id="top_nav" align="center"><div id="menuwrapper" align="center">
<ul id="menuBar" style="margin-bottom: 0px;">

<li><a class="trigger" href="/" name="top"> LAMurakami GitHub Pages</a>
    <ul>
    <li><a href="/About">     * About LAMurakami GitHub Pages </a></li>
    </ul>
</li>
<li><a class="trigger" href="http://sites.LAM1.us"> aka</a>
    <ul>
    <li><a href="https://lamurakami.gitlab.io/aws-efs">
    * https://lamurakami.gitlab.io/aws-efs</a></li>
    <li><a href="https://lamurakami.github.io/aws-efs">
    * https://lamurakami.github.io/aws-efs</a></li>
    <li><a href="https://ak20.lam1.us/github/aws-efs">
    * https://ak20.lam1.us/github/aws-efs</a></li>
    </ul>
</li><!-- END aka -->
<li><a class="trigger" href="#tail">        Contact me</a>
    <ul>
    <li><a href="/">                    * 907-978-0260</a></li>
    <li><a href="MailTo:GitHubPages@lamurakami.com">
                                            * GitHubPages@lamurakami.com</a></li>
    </ul>
</li><!-- END Contact me -->
</ul>
</div></div><br class="clearit"><!-- END top_nav menuwrapper menuBar -->
<span style="float: right;"><a href="#toc">
<img border="0" src="../ScrollDown.gif" alt="down" />Contents
<img border="0" src="../ScrollDown.gif" alt="down" /></a></span>
<h1 >&nbsp;

(AWS) Elastic File System (EFS)

</h1>
<br class="clearit">
<!-- start content -->
<div id="mw-content-text"><div lang="en" dir="ltr">

<ul><li><a rel="nofollow" href="https://ak20.lam1.us/a/Amazon_Web_Services_(AWS)">Amazon Web Services (AWS)</a> &#160; <a rel="nofollow" href="http://aws.lam1.us/About">Linux Apache MariaDB in the cloud</a> &#160; <a rel="nofollow" href="https://lamurakami.github.io/blog/2025/01/16/Modify_AWS_VPC_for_IPv6.html">Modify AWS VPC for IPv6</a></li>
<li>The aws-efs directory on my <a rel="nofollow" href="https://lamurakami.gitlab.io/aws-efs">GitLab</a>/<a rel="nofollow" href="https://lamurakami.github.io/aws-efs">GitHub</a> pages website <a rel="nofollow" href="https://gitlab.com/LAMurakami/lamurakami.gitlab.io/-/tree/master/aws-efs?ref_type=heads">GitLab project</a> <a rel="nofollow" href="https://github.com/LAMurakami/lamurakami.github.io/tree/master/aws-efs">GitHub repository</a></li>
<li><a rel="nofollow" href="https://ak20.lam1.us/Bk2/aws/efs/">Amazon Web Services (AWS) Elastic File System (EFS)</a> us-west-2 Oregon backup on ak20</li></ul>
<p>Early in my experience with AWS I discovered EFS and made use of a persistent parallel file system as a tool for launching a clone of the LAM Alaska website on demand.  I initially chose the us-west-2 Oregon region to operate in in 2017 and didn't use another region until 2024.  By that time success was defined as the ability to launch a fully functional clone of the LAM Alaska website with data from backups created from the main website within the last 24 hours.
</p>

<h2><span id="Amazon_Storage_Elastic_File_System_.28EFS.29_pricing"></span><span id="Amazon_Storage_Elastic_File_System_(EFS)_pricing">Amazon Storage Elastic File System (EFS) pricing</span></h2>
<p>Amazon EFS is built to scale on demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files.  You pay for usage that includes storage, reads, writes, storage class transitions based on the parameters of the filesystem.
</p><p>The pricing figures used are for the us-west-2 Oregon region which is my main region and one of the least expensive regions for AWS resources.
</p>
<h3><span id="Standard.2C_General_Purpose.2C_Bursting_EFS.2C_as_was_available_in_2017"></span><span id="Standard,_General_Purpose,_Bursting_EFS,_as_was_available_in_2017">Standard, General Purpose, Bursting EFS, as was available in 2017</span></h3>
<p>EFS Can be accessed by 1 to 1000s of EC2 instances concurrently.  A default Regional EFS can be accessed by instances in multiple AZs.  EFS can be used as network file system for on-premise servers too using AWS Direct Connect.
</p>
<ul><li>$0.30 per GB-month of storage used (Most expensive AWS storage option) - $3,600.00 / TB-Year
<ul><li>No charge for access from within the region</li>
<li>Every file system object is redundantly stored across multiple Availability Zones so it can survive one AZ failure.  Replicas are made within the region.  Data stored in AWS EFS stays in the region.</li>
<li>No public SLA for availability or durability.</li></ul></li>
<li>$1.50 per month (5 GB-month) discounted during Free-Tier</li>
<li>$0.01 per GB for EFS File Sync to the region</li></ul>
<h3><span id="One_Zone_and_Lifecycle_Managed_EFS_storage_classes">One Zone and Lifecycle Managed EFS storage classes</span></h3>
<p>Since I first started using EFS AWS has added One Zone and EFS storage classes.  There are also Performance and Throughput mode options that may not have been available in 2017 when I created my first EFS.
</p><p>After investigation into the new EFS options I decided that the Zz backup if moved to EFS from EBS would cost me less.  I did testing and forced almost all to go to the Infrequent Access Storage Class and tested that the daily backup does not bring unchanged target files back online and that access from the web interface is still an acceptable latency.  I was surprised at the cost of Elastic Throughput reads and writes to the One Zone Lifecycle Managed EFS which is significant and could really cost me if access isn't really infrequent.
</p><p>Although storage prices can be reduced by One Zone and Lifecycle Managed EFS storage classes the charges for throughput and data access could easily exceed any saving if access is to frequent.
</p>
<h3><span id="Regional_EFS_with_Elastic_Throughput_and_Lifecycle_Management">Regional EFS with Elastic Throughput and Lifecycle Management</span></h3>
<p>Standard storage cost is the same as in an EFS without Lifecycle Management but the Infrequent Access storage class can save nearly 95% and Archive storage class even more but Reads and Tiering costs could eat up the savings along with the minimum billable file size and minimum storage duration.
</p>
<h4><span id="Storage">Storage</span></h4>
<ul><li>$0.016 - Infrequent Access (GB-Month) - For inactive data accessed only a few times each quarter - $192 / TB-Year</li>
<li>$0.008 - Archive (GB-Month) - For inactive, long-lived data accessed only a few times each year or less - $96 / TB-Year</li></ul>
<h4><span id="Throughput_and_data_access">Throughput and data access</span></h4>
<ul><li>$0.06 - Writes (per GB transferred) - All storage classes</li>
<li>$0.03 - Reads (per GB transferred) - All storage classes</li>
<li>$0.01 - Reads / Tiering - Infrequent Access</li>
<li>$0.03 - Reads / Tiering - Archive</li>
<li>IA and Archive have a minimum billable file size of 128 KiB</li>
<li>Archive storage is charged for a minimum storage duration of 90 days, even if the data is deleted before 90 days.</li></ul>
<h3><span id="One_Zone_EFS_with_Elastic_Throughput_and_Lifecycle_Management">One Zone EFS with Elastic Throughput and Lifecycle Management</span></h3>
<p>Standard storage in a One Zone EFS is the smae price as Infrequent Access in a Regional EFS
</p>
<h4><span id="Storage_2">Storage</span></h4>
<ul><li>$0.16 - Standard (GB-Month - High-performance SSD storage for your most frequently-accessed data</li>
<li>$0.0133 - Infrequent Access (GB-Month) - Cost-optimized storage for inactive data accessed every few months - $159.60 / TB-Year</li></ul>
<h4><span id="Throughput_and_data_access_2">Throughput and data access</span></h4>
<ul><li>$0.06 - Writes (per GB transferred) - Elastic Throughput</li>
<li>$0.03 - Reads (per GB transferred) - Elastic Throughput</li>
<li>$0.01 - Reads / Tiering - Infrequent Access</li></ul>
<h2><span id="The_.28AWS.29_Elastic_File_System_.28EFS.29_metadata_directory_is_obsolete"></span><span id="The_(AWS)_Elastic_File_System_(EFS)_metadata_directory_is_obsolete">The (AWS) Elastic File System (EFS) metadata directory is obsolete</span></h2>
<p>As of August 2024 the <a rel="nofollow" href="https://gitlab.com/aws-lam/aws/-/blob/master/aws-efs-mount.bash?ref_type=heads">aws-efs-mount.bash script</a> replaces the (AWS) Elastic File System (EFS) metadata directory method used when I was addressing the IPv6 only workaround for git before putting my public git repos on GitLab that can be accessed with IPv6 as well as IPv4.
</p><p>Although I no longer use the metadata directory for initialization it is still the published location for this page.
</p>
<h2><span id="Mount_the_LAM_aws-efs_for_the_region_and_Availability_Zone_the_instance_was_launched_in">Mount the LAM aws-efs for the region and Availability Zone the instance was launched in</span></h2>
<h3><span id="AWS_LAM_VPC_Elastic_File_Systems">AWS LAM VPC Elastic File Systems</span></h3>
<p>In 2025 I expanded to three filesystems for each instance (where supported):
</p>
<ul><li>aws-efs A standard EFS, as was available in 2017, for each region
<ul><li>Performance mode: General Purpose - Throughput mode: Bursting - Lifecycle management: None</li></ul></li>
<li>aws-efs2 A Lifecycle Managed EFS for each region
<ul><li>Performance mode: General Purpose - Throughput mode: Elastic - Lifecycle management: 30 day Transition into Infrequent Access (IA), 90 day Transition into Archive, Transition back into Standard On first access</li></ul></li>
<li>aws-efs3 A One Zone Lifecycle Managed EFS for each Availability Zone (where supported)
<ul><li>Performance mode: General Purpose - Throughput mode: Elastic - Lifecycle management: 30 day Transition into Infrequent Access (IA), No Transition into Archive, Transition back into Standard On first access</li></ul></li></ul>
<p>All 32 REGIONs enabled for my AWS account, as of January 2025, support Standard and Lifecycle Management EFS but a number of Availability Zones including all for 11 REGIONs do not yet support One Zone Lifecycle Managed EFS.  I have detailed those with <b> No One Zone EFS</b> support in the <a rel="nofollow" href="https://lamurakami.github.io/blog/2025/02/07/I_created_63_One_Zone_Lifecycle_Managed_AWS_EFS.html">AWS Availability Zone subnet id</a> page I created while testing One Zone EFS for every Availability Zone that supports it.
</p><p>In 2017 when I created the EFS in the us-west-2 Oregon region it's usage in my LAM Alaska Clone project was based on the $0.30 / GB Month pricing which was the most expensive storage available but was extremely convenient.  EFS is mounted on Linux systems as a Network File System (NFS) with the ability to be accessed by thousands of instances and I don't have to manage the server.  It is persistent storage that doesn't require a running instance and although expensive I only pay for storage used form the very large capacity available.
</p><p>The systems using Lifecycle Management allow simple policies that can move data between standard storage and the new Infrequent Access (IA) and Archive storage classes that are less expensive than Standard storage.  The One Zone EFS is also cheaper than standard storage and is available to instances in a single Availability Zone instead of all in the region and includes the IA storage class for the cheapest still.
</p><p>I use "cloud-init query" to retrieve the REGION and Availability_Zone and then the <a rel="nofollow" href="https://gitlab.com/aws-lam/aws/-/blob/master/aws-efs-mount.bash?ref_type=heads">aws-efs-mount.bash script</a> to mount the filesystem.
</p><p>The cloud-init section to determine the region this instance was launched in and mount the EFS:
</p>
<pre> - echo
 - echo 'AWS LAM Get Availability_Zone from cloud-init values'
 - export Availability_Zone=$(cloud-init query availability-zone)
 - echo
 - echo 'AWS LAM Adding nfs4 mount to AWS LAM VPC Elastic File System'
 - export REGION=$(cloud-init query region)
 - /var/www/aws/<a rel="nofollow" href="https://gitlab.com/aws-lam/aws/-/blob/master/aws-efs-mount.bash?ref_type=heads">aws-efs-mount.bash</a>
</pre>
<p>The mount EFS section has been included in my cloud-init files since I became dependent on it's availability and ease of use but until 2024 when I started expanding to other regions the file system name was hard coded.  When a EFS is created within a region a unique file system ID is assigned and a DNS name like &lt;file system ID&gt;.efs.&lt;region&gt;.amazonaws.com is assigned but is only usable in the VPC where the EFS was created and by default is accessible only in that region.
</p><p>During the process of modifying my scripts to work in any region I reduced the EFS dependency to the ubuntu.tgz and ec2-user.tgz user resource files.  These files include some private resources including aws and duckdns credentials specific to an AWS LAM clone.
</p><p>Many of the other dependencies can be accessed from the us-west-2 Oregon EFS using the aws scp connectivity I set up during initialization.  The aws scp connectivity requires credentials which are in the ubuntu.tgz and ec2-user.tgz user resource files so mounting the EFS for the region is a requirement for accessing the us-west-2 Oregon EFS using the aws scp connectivity.
</p>
<h2><span id="Create_locate_database_for_nfs_mounted_efs">Create locate database for nfs mounted efs</span></h2>
<h3><span id="The_find_and_locate_commands_are_two_powerful_tools_to_search_for_one_or_more_particular_files">The find and locate commands are two powerful tools to search for one or more particular files</span></h3>
<p>The find command searches for files in real time, meaning that it will crawl the specified directory for your search query when you execute it.  The find command works well on fast local filesystems.  It can be real slow on large remote filesystems.
</p><p>The locate command is fast because it uses a database which is optimized for the locate searches.  It's main limitation is that it is dependent on the database and only reports on files that were present the last time the database was updated.  It also can’t perform as granular of a search as find, as it simply matches files based on their name, although it does accept complicated syntax such as regex.
</p><p>Both the find and locate commands have considerations and constraints when searching a remote filesystem.  Since an EFS is mounted using NFS and can be of massive size, how to search it is worth consideration.  I have decided to support searching the AWS EFS using locate and custom database files for the specific filesystems mounted.
</p>
<h3><span id="A_NFS_mounted_filesystem_is_not_included_in_a_locate_database_by_default.">A NFS mounted filesystem is not included in a locate database by default.</span></h3>
<p>There are some good reasons that NFS mounted filesystems are not included in the locate database.  One is that this very intensive IO operation could slam the NFS server and/or use a lot of the network bandwidth.  A second very related reason is that the NFS mounted filesystem is usually mounted on many remote systems.  This is the main reason for setting up a NFS server.  Bad as it might be to have one remote system add the entries for all the files on a remote filesystem in it's local database it is worse to have multiple systems do this.
</p>
<h3><span id="Create.2FUpdate_locate_database_upon_initialization"></span><span id="Create/Update_locate_database_upon_initialization">Create/Update locate database upon initialization</span></h3>
<h4><span id="Support_both_plocate_and_mlocate_utilities">Support both plocate and mlocate utilities</span></h4>
<p>Amazon Linux 2023 and Amazon Linux 2 are still using mlocate.  Ubuntu 24.04 Noble, 22.04 Jammy, and Debian all use the newer plocate utility.
</p><p>Depending on the Operating System and which locate utility is installed one of the following sets of programs is used during initialization.
</p>
<ul><li><a rel="nofollow" href="https://gitlab.com/aws-lam/no-ssl/-/blob/master/Create-update-efs-plocate-db.bash?ref_type=heads">Create-update-efs-plocate-db.bash</a>
<ul><li><a rel="nofollow" href="https://gitlab.com/aws-lam/no-ssl/-/blob/master/local_scripts/plocate-mnt-efs.sh?ref_type=heads">plocate-mnt-efs.sh</a></li>
<li><a rel="nofollow" href="https://gitlab.com/aws-lam/no-ssl/-/blob/master/local_scripts/plocate-mnt-efs2.sh?ref_type=heads">plocate-mnt-efs2.sh</a></li>
<li><a rel="nofollow" href="https://gitlab.com/aws-lam/no-ssl/-/blob/master/local_scripts/plocate-mnt-efs3.sh?ref_type=heads">plocate-mnt-efs3.sh</a></li></ul></li>
<li><a rel="nofollow" href="https://gitlab.com/aws-lam/no-ssl/-/blob/master/Create-update-efs-mlocate-db.bash?ref_type=heads">Create-update-efs-mlocate-db.bash</a>
<ul><li><a rel="nofollow" href="https://gitlab.com/aws-lam/no-ssl/-/blob/master/local_scripts/mlocate-mnt-efs.sh?ref_type=heads">mlocate-mnt-efs.sh</a></li>
<li><a rel="nofollow" href="https://gitlab.com/aws-lam/no-ssl/-/blob/master/local_scripts/mlocate-mnt-efs2.sh?ref_type=heads">mlocate-mnt-efs2.sh</a></li>
<li><a rel="nofollow" href="https://gitlab.com/aws-lam/no-ssl/-/blob/master/local_scripts/mlocate-mnt-efs3.sh?ref_type=heads">mlocate-mnt-efs3.sh</a></li></ul></li></ul>
<p>The Create-update-efs-{p,m}locate-db.bash scripts also create a /etc/profile.d/plocate.sh script that is sourced to include the {p,m}locate-db files in the LOCATE_PATH for all users.
</p><p>A long running instance such as my main aws instance can include the appropriate scripts in a daily-bakup job or in the cron.daily job to have the databases rebuilt daily.
</p>
<h2><span id="An_EFS_by_default_can_be_accessed_only_from_the_region_it_is_created_in">An EFS by default can be accessed only from the region it is created in</span></h2>
<p>By default EC2 instances running in multiple Availability Zones within the same AWS Region can access the Elastic File System (EFS) for that region.  An EFS can be created to serve only One Availability Zone but by default a mount target is created for each Availability Zone within the region.
</p><p>A Domain Name Service (DNS) name is assigned for each EFS which resolves to the IP address of the EFS mount target in the same Availability Zone as the EC2 instance querying the DNS.  Connectivity and DNS Name resolution is only available on the private IPv4 subnets of the VPC.
</p><p>I have decided not to use AWS Backup, EFS Replication or encryption on the filesystems I am creating in additional regions.  I am not sure any of these features were available when I created the Oregon EFS years ago.  The Lifecycle Management and One Zone features were introduced since I started using EFS.
</p>
<h2><span id="Create_an_EFS_in_a_region">Create an EFS in a region</span></h2>
<ul><li>Check 172.31.0.0/16 NFS for EFS inbound rule in Security Group</li>
<li>Create EFS security group with NFS for EFS inbound rule</li>
<li>Create EFS - aws-efs - use customize option</li>
<li>Attach EFS security group instead of default on second customize page for the interfaces for each Availability Zone in the region</li></ul>
<h2><span id="Populate_EFS_with_the_ubuntu.tgz_and_ec2-user.tgz_user_resource_files">Populate EFS with the ubuntu.tgz and ec2-user.tgz user resource files</span></h2>
<pre>alias ll='ls -lF --time-style=long-iso'
ll -daRh /mnt/efs{,/*,/*/*}
ll -daRh /mnt/efs{,/aws-lam1-ubuntu{,/*},/Amazon-Linux-2023{,/*}}
ll -daRh /mnt/efs/aws-lam1-ubuntu/ubuntu.t* /mnt/efs/Amazon-Linux-2023/ec2-user.t*
</pre>
<pre>sudo chown ubuntu:ubuntu /mnt/efs
sudo chown ec2-user:ec2-user /mnt/efs
sudo chown admin:admin /mnt/efs
sudo chown lam:staff /mnt/efs{,/aws-lam1-ubuntu{,/*},/Amazon-Linux-2023{,/*}}
</pre>
<pre>mkdir /mnt/efs/aws-lam1-ubuntu
</pre>
<pre>scp -p lam@aws:/mnt/efs/aws-lam1-ubuntu/ubuntu.t* \
/mnt/efs/aws-lam1-ubuntu
</pre>
<pre>mkdir /mnt/efs/Amazon-Linux-2023
</pre>
<pre>scp -p lam@aws:/mnt/efs/Amazon-Linux-2023/ec2-user.t* \
/mnt/efs/Amazon-Linux-2023
</pre>
<p>For every region other than the us-west-2 Oregon region only the ubuntu.t* and ec2-user.t* archives are installed.  The instance may update the mlocate.db or plocate.db files depending on which version of locate is used by the Operating System of the instance.  This only uses about 100K per region of aws-efs which should incur no monthly charges.
</p>
<h2><span id="The_original_us-west-2_Oregon_EFS_is_backed_up_to_my_main_server">The original us-west-2 Oregon EFS is backed up to my main server</span></h2>
<p>As of 2024 I am back up to 1 G of aws efs storage in the us-west-2 Oregon region costing me $0.30 per or $3.60 per year.  I also have a copy of most files in the aws s3://lamurakami bucket also in the us-west-2 Oregon region costing me an additional $0.28 per year.  In March and April of 2024 I had begun eliminating the efs copies but in April of 2024 I paused this when I realized I was using copies on the Bk2 backup of aws efs for initialization of qemu instances on the LAM Alaska LAN.
</p>
<ul><li><a rel="nofollow" href="https://ak20.lam1.us/Bk2/aws/efs/">Amazon Web Services (AWS) Elastic File System (EFS)</a> us-west-2 Oregon backup on ak20</li></ul>
<p>Files in this backup are used by <a rel="nofollow" href="https://ak20.lam1.us/a/QEMU_(Quick_Emulator)_(software)">QEMU (Quick Emulator) (software)|QEMU</a> instances on the LAM Alaska LAN during initialization.
</p>
<h2><span id="Log">Log</span></h2>
<ul><li>Published Page locations: <a rel="nofollow" href="https://lamurakami.gitlab.io/aws-efs/">https://lamurakami.gitlab.io/aws-efs/</a> <a rel="nofollow" href="https://lamurakami.github.io/aws-efs/">https://lamurakami.github.io/aws-efs/</a></li>
<li>As of February 2025 I have a standard EFS, as was available in 2017, and a Lifecycle Managed EFS for all 32 regions I have access to.
<ul><li>I have 63 One Zone Lifecycle Managed AWS EFS for each Availability Zone where it is supported that I have access to.</li></ul></li>
<li>On Saturday, August 24, 2024 I enabled my 30th. AWS region but EFS is not yet available in the brand new Asia Pacific (Malaysia) region.
<ul><li>I had just completed the change to use aws-efs-mount.bash instead of the (AWS) Elastic File System (EFS) metadata directory method to mount the AWS LAM VPC EFS unique filesystem for the region the instance was launched in.</li></ul></li>
<li>On <a rel="nofollow" href="https://ak20.lam1.us/Zz/z24/z2407/2024-07-28/Daily-ak19.txt">Sunday, July 28, 2024</a> I published to <a rel="nofollow" href="https://lamurakami.gitlab.io/aws-efs">GitLab Pages</a> aws well as <a rel="nofollow" href="https://lamurakami.github.io/aws-efs">GitHub Pages</a>.</li>
<li>On <a rel="nofollow" href="https://ak20.lam1.us/Zz/z24/z2407/2024-07-10/Daily-ak19.txt">Wednesday, July 10, 2024</a> I finished distributing the new ubuntu.t* and ec2-user.t* archives to the aws-efs of all 29 regions.</li>
<li>In July of 2024 I am increasing the aws-efs usage from about 40K to about 100K per region.
<ul><li>The ubuntu.t* and ec2-user.t* archives now include the git repo including the git@ak20:user-bash ak20 remote definition.</li>
<li>I wouldn't incur even a half penny a month until at least 15M at the $0.30 per GB-Month rate of aws-efs in the us-west-2 Oregon region.</li></ul></li>
<li>On Friday, June 21, 2024 I replaced REGION.bash with the "cloud-init query region" command.</li>
<li>Sunday, February 25, 2024 Published this page for the first time.</li>
<li>As of <a rel="nofollow" href="https://ak20.lam1.us/Zz/z24/z2402/2024-02-18/Daily-ak19.txt">Sunday, February 18, 2024</a> I have three regions I have created an EFS in and can launch a full clone on an Ubuntu ARM instance without a public IP address as long as aws.lam1.us is available.
<ul><li>I have no intention to replicate anything but the ubuntu.tgz and ec2-user.tgz files on an EFS in every region I will operate.  I can use aws S3 which is accessible by any region and for additional instances can use aws.lam1.us as a source.  For instances that do have a public IPv4 address I can use github for the source of repos I have made public there.</li>
<li>My EFS usage in Oregon had grown to over a GB which costs me over $0.30 a month.  Replicating it to all 28 regions would approach $10.00 a month cost.  I tested adding the value to the instance tags so I won't have to make 28 versions of each of the over 15 cloud-init files.  Additional name pair tags could be used to reduce the number of cloud-init files.  I dropped using the EFS tag because it was easy after I had the aws credentials which is one of the secrets I want on the private EFS.</li></ul></li></ul>
<!-- 
NewPP limit report
Cached time: 20250216042909
Cache expiry: 86400
Reduced expiry: false
Complications: [show‐toc]
CPU time usage: 0.010 seconds
Real time usage: 0.010 seconds
Preprocessor visited node count: 77/1000000
Post‐expand include size: 100/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/100
Expensive parser function count: 0/100
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 0/5000000 bytes
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 -total
-->

<!-- Saved in parser cache with key wikidb:pcache:idhash:485-0!canonical and timestamp 20250216042909 and revision id 8591. Rendering was triggered because: edit-page
 -->
</div>
</div>
<!-- end content -->
<div id="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" style="display:none" /><div lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><label for="toctogglecheckbox"></label></div>
<div data-nosnippet="">Retrieved from "<a dir="ltr" href="https://ak20.lam1.us/A/index.php?title=(AWS)_Elastic_File_System_(EFS)&amp;oldid=8591">https://ak20.lam1.us/A/index.php?title=(AWS)_Elastic_File_System_(EFS)&amp;oldid=8591</a>"</div>
<ul>
<li><a href="#Amazon_Storage_Elastic_File_System_(EFS)_pricing">1 Amazon Storage Elastic File System (EFS) pricing</a>
<ul>
<li><a href="#Standard,_General_Purpose,_Bursting_EFS,_as_was_available_in_2017">1.1 Standard, General Purpose, Bursting EFS, as was available in 2017</a></li>
<li><a href="#One_Zone_and_Lifecycle_Managed_EFS_storage_classes">1.2 One Zone and Lifecycle Managed EFS storage classes</a></li>
<li><a href="#Regional_EFS_with_Elastic_Throughput_and_Lifecycle_Management">1.3 Regional EFS with Elastic Throughput and Lifecycle Management</a>
<ul>
<li><a href="#Storage">1.3.1 Storage</a></li>
<li><a href="#Throughput_and_data_access">1.3.2 Throughput and data access</a></li>
</ul>
</li>
<li><a href="#One_Zone_EFS_with_Elastic_Throughput_and_Lifecycle_Management">1.4 One Zone EFS with Elastic Throughput and Lifecycle Management</a>
<ul>
<li><a href="#Storage_2">1.4.1 Storage</a></li>
<li><a href="#Throughput_and_data_access_2">1.4.2 Throughput and data access</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#The_(AWS)_Elastic_File_System_(EFS)_metadata_directory_is_obsolete">2 The (AWS) Elastic File System (EFS) metadata directory is obsolete</a></li>
<li><a href="#Mount_the_LAM_aws-efs_for_the_region_and_Availability_Zone_the_instance_was_launched_in">3 Mount the LAM aws-efs for the region and Availability Zone the instance was launched in</a>
<ul>
<li><a href="#AWS_LAM_VPC_Elastic_File_Systems">3.1 AWS LAM VPC Elastic File Systems</a></li>
</ul>
</li>
<li><a href="#Create_locate_database_for_nfs_mounted_efs">4 Create locate database for nfs mounted efs</a>
<ul>
<li><a href="#The_find_and_locate_commands_are_two_powerful_tools_to_search_for_one_or_more_particular_files">4.1 The find and locate commands are two powerful tools to search for one or more particular files</a></li>
<li><a href="#A_NFS_mounted_filesystem_is_not_included_in_a_locate_database_by_default.">4.2 A NFS mounted filesystem is not included in a locate database by default.</a></li>
<li><a href="#Create/Update_locate_database_upon_initialization">4.3 Create/Update locate database upon initialization</a>
<ul>
<li><a href="#Support_both_plocate_and_mlocate_utilities">4.3.1 Support both plocate and mlocate utilities</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#An_EFS_by_default_can_be_accessed_only_from_the_region_it_is_created_in">5 An EFS by default can be accessed only from the region it is created in</a></li>
<li><a href="#Create_an_EFS_in_a_region">6 Create an EFS in a region</a></li>
<li><a href="#Populate_EFS_with_the_ubuntu.tgz_and_ec2-user.tgz_user_resource_files">7 Populate EFS with the ubuntu.tgz and ec2-user.tgz user resource files</a></li>
<li><a href="#The_original_us-west-2_Oregon_EFS_is_backed_up_to_my_main_server">8 The original us-west-2 Oregon EFS is backed up to my main server</a></li>
<li><a href="#Log">9 Log</a></li>
</ul>
</div>
</body></html>
